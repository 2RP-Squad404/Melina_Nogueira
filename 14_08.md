# Relatório de Estudos

**Nome do Estagiário:** Melina Nogueira  
**Data:** 14/08/2024

## **Índice**  
1. **[Virtualização](#virtualização)**  
    1.1. **[Kubernetes](#kubernetes)**
2. **[Linguagens e Frameworks](#linguagens-e-frameworks)**  
    2.1. **[Python](#python)**  
    2.2. **[Apache Beam](#apache-beam)**  
    2.3. **[Google Dataflow](#google-dataflow)**  
    2.4. **[Apache Airflow](#apache-airflow)**

## Resumo dos módulos

### Virtualização 

#### **Kubernetes**

**O que é**  
Plataforma de código aberto para [orquestração de containers](#orquestracao-de-containers "processo de automatização da rede e do gerenciamento de contêineres") que automatiza a implantação, o dimensionamento e a operação de aplicativos em containers.

**Características**
- **Orquestração em containers:** Automatiza o gerenciamento de containers em um cluster, incluindo a implantação, a atualização, o balanceamento de carga e a recuperação de falhas;
- **Escalabilidade:** Permite o dimensionamento automático de aplicações;
- **Desdobramento e atualização:** Facilita a implantação contínua e a atualização de aplicativos;
- **Auto-recuperação:** Monitora a saúde dos containers e dos nós do cluster, substituindo automaticamente os containers com falhas e redistribuindo as cargas se necessário;
- **Gerenciamento de configuração e segredos:** Oferece mecanismos para gerenciar configurações e segredos de forma segura;
- **Serviços e networking:** Fornece abstração de serviços, balanceamento de carga e descoberta de serviços dentro do cluster.

**Componentes**
1. **Pod:** Menor unidade de implantação no Kubernetes, pode conter um ou mais containers que compartilham armazenamento e rede;
2. **Node:** Uma máquina que executa os Pods e fornece os recursos necessários para a execução dos containers;
3. **Cluster:** Conjunto de Nodes que trabalham juntos para executar as aplicações;
4. **Master Node:** Nó central que coordena o cluster, gerenciando a comunicação entre os Nodes e controlando a execução dos Pods;
5. **Deployment:** Define a forma que um conjunto de pods deve ser executado e gerenciado;
6. **Service:** Abstração que define um conjunto de pods e fornece uma forma de expô-los como um serviço de rede;
7. **ConfigMap e Secret:** Objetos que permitem separação de configurações e informações sensíveis;
8. **Ingress:** Gerencia o acesso externo aos serviços no cluster, balanceando a carga e roteando o tráfego.

**Desafios**
- Complexidade em configurar e gerenciar Kubernetes;
- Sobrecarga de recursos devido à camada de abstração;
- Curva de aprendizado devido ao tempo e à dificuldade de aprender a tecnologia.

### Linguagens e Frameworks

#### **Python**
**O que é**  
Linguagem de programação de [alto nível](#alto-nivel "Maior concentração de códigos e instruções, promove produtividade"), [interpretada](#interpretada "Conversão analisando os códigos linha por linha") e de propósito geral, conhecida por sua simplicidade e legibilidade.

**Características**
- Sintaxe simples e legível, sendo fácil de ler e escrever;
- Interpretação e portabilidade: Por ser uma linguagem interpretada, o Python pode ser executado em várias [plataformas](#plataformas "Windows, macOS e Linux");
- Possui várias bibliotecas e [frameworks](#frameworks "Automatizar a implementação de diversas tarefas e promover uma estrutura para desenvolvimento de aplicações") que facilitam o desenvolvimento de aplicações;
- Comunidade ativa, contribuindo com novos pacotes, recursos e suporte.

**Aplicações**
- Desenvolvimento Web: Utiliza frameworks como Django e Flask;
- Data Science e Machine Learning: Utiliza bibliotecas como NumPy, Pandas, Matplotlib, SciPy, Scikit-learn e TensorFlow;
- Automação e Scripting: Python é utilizado para automatizar tarefas repetitivas, escrever scripts para gerenciar sistemas e automatizar processos;
- Desenvolvimento de Software: Utilizado para criar aplicações de software;
- Inteligência Artificial e Deep Learning: Utilização de frameworks como TensorFlow e PyTorch permitem o desenvolvimento de modelos de IA e deep learning;
- Computação Científica e Pesquisa: Python é muito utilizado em pesquisas acadêmicas e científicas devido à capacidade em manipular e analisar grandes conjuntos de dados e sua integração com outras ferramentas científicas;
- Desenvolvimento de Jogos: Há bibliotecas como Pygame que permitem o desenvolvimento de jogos 2D simples;
- Desenvolvimento de Aplicativos Desktop: Bibliotecas como Tkinter e PyQt são usadas para criar interfaces gráficas de usuário (GUI) para aplicativos desktop.

#### **Apache Beam** 

**O que é**  
É um modelo de programação unificado e de código aberto para processamento de dados em lote e em tempo real, que permite a definição e execução de pipelines.

**Características**
- Modelo unificado para processamento de dados em lote e streaming;
- Portabilidade: Permite que você escreva pipelines e os execute em diferentes sistemas de execução, como Google Dataflow, Apache Flink, Apache Spark, e outros;
- API flexível: Suporte a várias linguagens, como Java, Python e Go;
- Transformações: Conjunto de transformações para manipular dados, como map, filter, groupByKey e windowing;
- Janelas e Watermarks: Suporte para processamento de eventos em tempo real, incluindo o conceito de janelas e marcações de água para lidar com dados fora de ordem.

**Usos**
- ETL (Extract, Transform, Load): Processar e transformar grandes volumes de dados antes de carregar um data warehouse ou data lake;
- Processamento de dados em tempo real: Analisar e responder a dados de streaming em tempo real;
- Agregação e Análises: Realizar cálculos complexos e agregações em grandes conjuntos de dados.

**Desafios**
- Curva de aprendizado;
- Desempenho do executor: O desempenho da pipeline varia de acordo com o sistema de execução.

#### **Google Dataflow**

**O que é**  
Executa pipelines de dados criados com Apache Beam, fornecendo uma plataforma para o processamento de dados em lote e streaming com escalabilidade e gerenciamento.

**Características**
- Não requer gerenciamento de infraestrutura;
- Escalabilidade automática otimiza custos e desempenho;
- Integração com outros serviços do Google;
- Oferece ferramentas de monitoramento e visualização para acompanhar as pipelines;
- Modelo de preços baseado no tempo de processamento e volume de dados.

**Usos**
- Processamento de logs: Processar e analisar logs gerados por aplicações e serviços;
- Análise de dados em tempo real;
- Data Integration: Integrar e transformar dados de várias fontes.

**Desafios**
- Custo;
- Dependência do Google Cloud: Opera dentro do ecossistema do Google Cloud.

#### **Apache Airflow**

**O que é**  
Plataforma de código aberto para orquestração e automação de [workflows](#workflows "Fluxo de trabalho: sequência de tarefas ou processos que precisam se executados em ordem") de dados.

**Características**
- Definição de Workflow como código: Utilização do Python para definir workflows, permitindo a criação de DAGs;
- Oferece interface web para visualização, monitoramento e gerenciamento dos workflows e das execuções;
- Suporta agendamentos de tarefas baseados em [cron](#cron "Agendamento por horários ou intervalos específicos"), [intervalos](#intervalos "Agendamento baseado em intervalos de tempo regulares") ou [triggers](#triggers "Executa tarefa quando acontecem eventos ou conforme as condições");
- Permite a execução paralela de tarefas e pode ser escalado para trabalhar com grandes volumes de tarefas e dados;
- Inclui mecanismos para retry automático de tarefas com falhas e recuperação de erros;
- Possui operadores e [hooks](#hooks "Componentes que permitem a interação direta com serviços ou sistemas externos") para integração com uma ampla gama de sistemas e serviços.

**Componentes**
1. **DAG:**  
    - Directed: Fluxo de trabalho se dá apenas em uma direção;  
    - Acyclic: A execução não entrará em um laço de repetição;  
    - Graphs: Ferramenta matemática com nós e arestas responsáveis por conectar esses nós.
2. **Operadores:** Componentes que definem as ações a serem executadas em cada tarefa;
3. **Tasks:** Unidades de trabalho definidas numa DAG;
4. **Scheduler:** Responsável por acionar as tarefas baseadas no cronograma definido e garantir que as tarefas sejam executadas;
5. **Executor:** Lida com a execução real das tarefas, podendo ser configurado para usar diferentes backends;
6. **Web Interface:** Interface gráfica para visualizar, gerenciar DAGs e monitorar o status das tarefas e visualizar logs de execução.

**Usos**
- ETL (Extract, Transform, Load);
- Data Pipeline Management: Coordenação de workflows complexos que envolvem múltiplas etapas e dependências entre tarefas;
- Automação de processos: Automatização de tarefas recorrentes;
- Integração de Sistemas: Orquestração de integração entre diferentes sistemas e serviços.

---

### **Recursos Utilizados:**  
- GitHub  
- [O que é orquestração de contêineres?](https://aws.amazon.com/pt/what-is/container-orchestration/)  
- [What are Frameworks in Python? Know Top 5 Python Frameworks](https://www.simplilearn.com/python-frameworks-article)  
- [Qual a diferença entre linguagens compiladas e interpretadas?](https://www.softwarestart.com.br/diferenca-entre-linguagens-compiladas-e-interpretadas)  
- [Airflow - Entendendo os DAGs](https://www.alura.com.br/artigos/airflow-entendendo-dags)

---

### **Desafios Encontrados:**  
- Compreender alguns conceitos e termos específicos.

### **Próximos Passos:**  
- Concluir as trilhas de Data Science e praticar.

---

