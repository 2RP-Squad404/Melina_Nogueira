# Relatório de Estudos

**Nome do Estagiário:** Melina Nogueira  
**Data:** 20/08/2024

## **Índice**  
1. **[Computação em Nuvem](#computação-em-nuvem)**

    1.1. **[Google Cloud Dataflow](#google-cloud-dataflow)**
    
    1.2. **[Google Cloud Dataproc](#google-cloud-dataproc)**

    1.3. **[Google Cloud Composer](#google-cloud-composer)**

    1.4. **[Google Cloud Functions](#google-cloud-functions)**
    
2. **[CI/CD](#cicd)**

3. **[Linux/Shell](#linuxshell)**
---

## Resumo dos módulos

### Computação em Nuvem

#### **Google Cloud Dataflow**

**O que é**

Serviço para o processamento e análise de dados em larga escala, projetado para executar pipelines de processamento de dados em tempo real e em [batch](#batch "Arquivo texto contendo linhas com comandos que podem ser executados sequencialmente").

**Características**
- Modelo de programação: permite criar pipelines de processamento de dados usando um modelo de programação unificado para processamento em batch e streaming;
- Gerenciamento automático de recursos necessários para a execução de pipelines;
- Escalabilidade: escala os recursos automaticamente para atender à demanda;
- Flexibilidade de processamento: suporta processamento em tempo real e em batch, permitindo definição e execução de pipelines para diferentes tipos de processamento de dados;
- Fácil integração com outros serviços do Google Cloud;
- Oferece interface para monitorar a execução das pipelines, visualizar métricas e logs, e diagnosticar problemas.

**Componentes**
1. Pipeline: define o fluxo de dados e as tranformações que deverá ser aplicada aos dados, composto por transformações que processam os dados de entrada e produzem os dados de saída;
2. Transformações: operações aplicadas aos dados, como filtragem, agregação e junção;
3. PCollections: coleção de dados que são manipulados pelas transformações;
4. Data Sources e Sinks: fontes e destinos dos dados. Os data sources, ou fontes, podem ser  serviços de armazenamento, banco de dados ou fluxos de dados em tempo real. Já os sinks, ou destinos, são locais onde os dados processados são utilizados ou armazenados;
5. Workers: instâncias que executam as tarefas de processamento das pipelines.

**Desafios**
- Complexidade de configuração e otimização de pipelines;
- Custo conforme quantidade de dados processados e recursos utilizados;
- Curva de aprendizado requer familiaridade com o modelo de programação do Apache Beam.

#### **Google Cloud Dataproc**

**O que é**

Serviço para processamento de dados em larga escala, utilizando Apache Hadoop e Apache Spark.

**Características**
- Gerenciamento de clusters: gerencia automaticamente o ciclode vida dos clusters, como a criação, configuração, escalabilidade e terminação;
- Integração com Google Cloud;
- Escalabilidade: permite escalabilidade automática e manual dos clusters para atender diferentes cargas de trabalho;
- Eficiência de custo: criação de clusters sob demanda e execução de trabalhos em [cluster efêmeros](#cluster-efemeros "Ambientes temporários, criados para realizar tarefas específicas e destruídos depois da conclusão das tarefas");
- Suporte a Frameworks: permite utilização de Frameworks de processamento de dados, como Apache Hadoop, Apache Spark, Apache Hive e Apache HBase;
- Monitoramento e logs: oferece ferramentas para monitorar o desempenho dos clusters e acessar logs de execução.

**Componentes**
1. Cluster: conjunto de máquinas virtuais que executam tarefas de processamento de dados;
2. Jobs: tarefas de processamento de dados enviadas para o cluster;
3. Nodes: máquinas virtuais dentro do cluster que executam as tarefas;
4. Inicializadores de Cluster: scripts executados quando um cluster é criado, permitindo a configuração dele;
5. Templates de trabalho: permitem definição e reuso das configurações para jobs.

**Desafios**
- Complexidade de frameworks: exige conhecimento especializado para configurar e otimizar;
- Gerenciamento de dados: possível perca de eficiência e desempenho devido a manipulação e gerenciamento de grandes volumes de dados;
- Custo: aumento de despesas devido à execução de clusters de longa duração ou a execução intensiva de tarefas.

#### **Google Cloud Composer**

**O que é**  
Serviço gerenciado de orquestração de workflows baseado em Apache Airflow, facilita a criação, execução e monitoramento de pipelines de dados e workflows.

**Características**
- Baseado em Apache Airflow, proporciona flexibilidade e poderosas capacidades de automação para gerenciar pipelines de dados;
- Gerenciamento automatizado dos recursos necessários para executar workflows;
- Integração com Google Cloud;
- Escalabilidade, ajuda no gerenciamento dos recursos necessários para a execução dos workflows;
- Interface do usuário para visualizar, monitorar e gereciar os workflows;
- Segurança e Compliance, oferece suporte a práticas de segurança e [compliance](#compliance "Conjunto de práticas, normas, procedimentos e políticas").

**Componentes**
1. DAG (Directed Acyclic Graph): define a sequência e as dependências das tarefas dentro de um pipeline;
2. Tarefas: unidades de trabalho definidas dentro de um DAG, cada tarefa representa uma operação ou um passo numa pipeline;
3. Executores: executa as tarefas definidas no DAG; 
4. Operadores: encapsulam a lógica para interagir com sistemas e serviços diferentes;
5. Sensors: tipo de operador que aguarda uma ocorrência de um evento ou a disponibilidade de algum recurso antes de prosseguir com a execução;
6. Hooks: interface para conectar e interagir com sistemas externos, como bancos de dados e APIs.

#### **Google Cloud Functions**

**O que é**  
Serviço de computação serverless que permite a execução de um código em resposta a eventos, sem precisar gerenciar ou provisionar servidores.

**Características**
- Serveless: não precisa gerenciar infraestrutura, pois ele faz isso automaticamente;
- Escalabilidade automática;
- Execução baseada em eventos;
- Integração com o Google Cloud;
- Tempo de execução rápido;
- Preços baseados em uso: o custo é baseado no número de execuções e no tempo de computação utilizado. 

**Componentes**
- Funções: unidades de código que são executadas em resposta a eventos;
- Gatilhos (triggers): eventos que acionam a execução de uma função;
- Eventos: dados acompanham o gatilho e são passados para a função quando ela é executada;
- Ambiente de execução: é o ambiente onde as funções são executadas, ele é configurado automaticamente;
- Configurações de funções: definição de variáveis de ambiente, permissões e entre outras configurações para o funcionamento das funções.

**Desafios**
- Limitações de tempo de execução;
- Estado e persistência: pode exigir serviços adicionais para armazenar dados;
- Gerenciamento de dependências: apesar do gerenciamento automático da infraestrutura, ainda é necessário gerenciar e configurar as dependências da função.


### CI/CD

**O que é**
[CI/CD](#ci-cd "Continuous Integration/Continuous Delivery") são práticas no desenvolvimento de softwares que visam melhorar a qualidade e eficiência do ciclo de vida do desenvolvimento.

**Conceitos**
- CI (Integração Contínua)
    - Definição: integrar alterações de código em um repositório compartilhado;
    - Objetivo: reduzir risco de integração de código e garantir que as alterações sejam compatíveis com o código existente;
    - Processo:
        - Os devs realizam as alterações e enviam para o repositório;
        - O CI detecta as mudanças, executa uma build do código e realiza testes automatizados;
        - Os relatórios são gerados para indicar sucesso ou falha nos testes, garantido uma rápida correção de problemas.
- CD (Entrega Contínua)
    - Definição: prática de manter o código em estado pronto para ser implementado a qualquer momento;
    - Objetivo: garantir que o código sempre esteja pronto para ser lançado em produção e com menos esforço manual;
    - Processo:
        - Após o processo de CI, o código é automaticamente implantado em um [ambiente de staging](#ambiente-de-staging "Ambiente de teste simulando a realidade");
        - Testes são executados no staging para garantir que o software está pronto para produção;
        - Se os testes forem bem-sucedidos, o código é considerado pronto para ser implantado.

- CD (Deploy Contínuo):
    - Definição: extensão de entrega contínua, onde as alterações de código são automaticamente implantadas após passar pelo staging;
    - Objetivo: automatizar o processo de implantação;
    - Processo:
        - O código é integrado e testado automaticamente;
        - Se os testes forem aprovados, acontece a implementação sem intervenção manual.

**Desafios**
- Complexidade de configuração;
- Gerenciamento de dependências;
- Segurança para evitar falhas e vulnerabilidades.

### Linux/Shell

**O que é**

Shell Script é um script de computador executado por shell Unix ou Linux. O Shell Script são arquivos de texto que contêm diversos comandos que o shell pode interpretar e executar. Utilizado para automatizar tarefas e simplificar execução de comandos complexos.

**Conceitos**
- Shell: programa que fornece uma interface de linha de comando para interagir com o sistema operacional;
- Script: arquivo que contém os comandos;
- Comandos: instruções para executar tarefas no sistema operacional;
- Variáveis: usadas para armazenar dados que podem ser utilizados em comandos  operações;
- Loops e Condicionais: estruturas de controle, como loops (for, while) e condicionais (if, else), utilizados para executar comandos repetidamente ou tomar decisões;
- Funções: permite encapsulamento de um conjunto de comandos em um bloco reutilizável.

**Comandos**
- Listar arquivos
```bash
ls 
```

- Buscar padrões
```bash
grep
```

- Copiar arquivos
```bash
cp
```

- Script de backup

    `tar:` cria um arquivo no formato tar, que permite agrupar vários arquivos em um único arquivo;

    `-c:` cria um novo arquivo tar;

    `-z:` comprime o arquivo usando gzip;

    `-f:` especifica o nome do arquvio tar.

    ```bash
    tar -czf /backup/meuarquivo_backup_$(date +%F).tar.gz /     meuarquivo
    ```

- Verificar espaço em disco

    `echo:` exibe uma mensagem na saída;

    `df:` exibe informações sobre o uso do sistema de arquivos;

    `h:` faz exibir o espaço em disco em unidades como KB, MB, GB.

    ```bash
    echo "Espaço disponível em disco:"
    df -h
    ```

- Monitoramento de processos

    `pgrep:` procura um processo com o nome, no caso ele vai pesquisar o processo_exemplo, se ele achar ele retorna o ID do processo;

    `> /dev/null:` descarta a saída, que no caso é o ID, para que ele nao seja exibido no terminal;

    `fi:` indica o final da estrutura if.

    ```bash
    if pgrep "processo_exemplo" > /dev/null
    then
        echo "O processo está em execução."
    else
        echo "O processo não está em execução."
    fi
    ```
---

### **Recursos Utilizados:**  
- GitHub  
- [Compliance: conceito, tipos, benefícios e como colocar em prática](https://www.aurum.com.br/blog/compliance/)
- [Automações de teste com projetos e clusters efêmeros](https://www.mongodb.com/pt-br/docs/atlas/cli/v1.13/atlas-cli-ephemeral-cluster/)
- [Diferentes ambientes: Development, Testing, Staging e Production](https://klauslaube.com.br/2011/03/07/diferentes-ambientes.html)

---

### **Desafios Encontrados:**  
- Compreender alguns termos e entender os comandos do shell.

### **Próximos Passos:**  
- Continuar estudando principalmente para conseguir praticar.
---
